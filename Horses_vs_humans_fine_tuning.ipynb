{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar_10_fine_tuning",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "1tRBeUodgrrq",
        "RFkeXcghlU6a"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tRBeUodgrrq"
      },
      "source": [
        "# 1.0 Useful classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B47RhEPVhIxP"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os\n",
        "\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlHvOPCsZ5yC"
      },
      "source": [
        "## 1.1 HDF5DatasetWriter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-IfBT8dhDiD"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
        "    \"\"\"\n",
        "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
        "    \n",
        "    Args:\n",
        "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
        "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
        "    then dims=(70000, 784).\n",
        "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
        "    datakey: The optional dataKey is the name of the dataset that will store\n",
        "    the data our algorithm will learn from.\n",
        "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
        "    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # check to see if the output path exists, and if so, raise\n",
        "    # an exception\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"The supplied `outputPath` already \"\n",
        "        \"exists and cannot be overwritten. Manually delete \"\n",
        "        \"the file before continuing.\", outputPath)\n",
        "\n",
        "    # open the HDF5 database for writing and create two datasets:\n",
        "    # one to store the images/features and another to store the\n",
        "    # class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\",compression='gzip')\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
        "\n",
        "    # store the buffer size, then initialize the buffer itself\n",
        "    # along with the index into the datasets\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add(self, rows, labels):\n",
        "    # add the rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    # check to see if the buffer needs to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "\n",
        "  def flush(self):\n",
        "    # write the buffers to disk then reset the buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check to see if there are any other entries in the buffer\n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) > 0:\n",
        "      self.flush()\n",
        "\n",
        "    # close the dataset\n",
        "    self.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGZswDVOaC3C"
      },
      "source": [
        "## 1.2 Image to Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgmBX1ST1F8U"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcDGt0BnaGXY"
      },
      "source": [
        "## 1.3 AspectAware"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-FgPH1NuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "# useful class to help the resize of images\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgdXcA7aPfT"
      },
      "source": [
        "## 1.4 Mean preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgq3q5uLZj-l"
      },
      "source": [
        "Let’s get started with the mean pre-processor. We will learn how to convert an image\n",
        "dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and Blue pixel intensities across all images in the training dataset. Now that we have these averages, we are going to perform a pixel-wise subtraction of these values from our input images as a **form of data normalization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfbg-dCsUEuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class MeanPreprocessor:\n",
        "\tdef __init__(self, rMean, gMean, bMean):\n",
        "\t\t# store the Red, Green, and Blue channel averages across a\n",
        "\t\t# training set\n",
        "\t\tself.rMean = rMean\n",
        "\t\tself.gMean = gMean\n",
        "\t\tself.bMean = bMean\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# split the image into its respective Red, Green, and Blue\n",
        "\t\t# channels\n",
        "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n",
        "\n",
        "\t\t# subtract the means for each channel\n",
        "\t\tR -= self.rMean\n",
        "\t\tG -= self.gMean\n",
        "\t\tB -= self.bMean\n",
        "\n",
        "    # Keep in mind that OpenCV represents images in BGR order\n",
        "\t\t# merge the channels back together and return the image\n",
        "\t\treturn cv2.merge([B, G, R])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COixYybsaWyp"
      },
      "source": [
        "## 1.5 Patch preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxZ6UF7ahNo"
      },
      "source": [
        "The PatchPreprocessor is responsible for randomly sampling MxN regions of an image during the training process. We apply patch preprocessing when the spatial dimensions of our input images are larger than what the CNN expects – this is a common technique to help reduce overfitting, and is, therefore, **a form of regularization**. Instead of using the entire image during training, we instead crop a random portion of it and pass it to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPeYByPoa6C4"
      },
      "source": [
        "As you will see, we will construct an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256x256 pixels. However, the AlexNet architecture that we’ll be implementing later in this lesson can only accept images of size 227x227 pixels. This is an excellent opportunity to perform data augmentation by randomly cropping a 227x227 region from the 256x256 image during training using PatchPreprocessor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm2bRAs2UKHj"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "class PatchPreprocessor:\n",
        "\tdef __init__(self, width, height):\n",
        "\t\t# store the target width and height of the image\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# extract a random crop from the image with the target width\n",
        "\t\t# and height\n",
        "\t\treturn extract_patches_2d(image, (self.height, self.width),\n",
        "\t\t\tmax_patches=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gft-PE3aJvZ"
      },
      "source": [
        "## 1.6 Crop preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFW6NqV2eSYY"
      },
      "source": [
        "Next, we need to define a CropPreprocessor responsible for computing the 10-crops for oversampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image + the center region and then take their corresponding horizontal flips, for a total of ten samples per input image.\n",
        "\n",
        "These ten samples will be passed through the CNN, and then the probabilities averaged.\n",
        "Applying this over-sampling method tends to include 1-2 percent increases in classification accuracy (and in some cases, even higher)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc0kftNdR2lk"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class CropPreprocessor:\n",
        "\tdef __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, whether or not\n",
        "\t\t# horizontal flips should be included, along with the\n",
        "\t\t# interpolation method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.horiz = horiz\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# initialize the list of crops\n",
        "\t\tcrops = []\n",
        "\n",
        "\t\t# grab the width and height of the image then use these\n",
        "\t\t# dimensions to define the corners of the image based\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tcoords = [\n",
        "\t\t\t[0, 0, self.width, self.height],\n",
        "\t\t\t[w - self.width, 0, w, self.height],\n",
        "\t\t\t[w - self.width, h - self.height, w, h],\n",
        "\t\t\t[0, h - self.height, self.width, h]]\n",
        "\n",
        "\t\t# compute the center crop of the image as well\n",
        "\t\tdW = int(0.5 * (w - self.width))\n",
        "\t\tdH = int(0.5 * (h - self.height))\n",
        "\t\tcoords.append([dW, dH, w - dW, h - dH])\n",
        "\n",
        "\t\t# loop over the coordinates, extract each of the crops,\n",
        "\t\t# and resize each of them to a fixed size\n",
        "\t\tfor (startX, startY, endX, endY) in coords:\n",
        "\t\t\tcrop = image[startY:endY, startX:endX]\n",
        "\t\t\tcrop = cv2.resize(crop, (self.width, self.height),\n",
        "\t\t\t\tinterpolation=self.inter)\n",
        "\t\t\tcrops.append(crop)\n",
        "\n",
        "\t\t# check to see if the horizontal flips should be taken\n",
        "\t\tif self.horiz:\n",
        "\t\t\t# compute the horizontal mirror flips for each crop\n",
        "\t\t\tmirrors = [cv2.flip(c, 1) for c in crops]\n",
        "\t\t\tcrops.extend(mirrors)\n",
        "\n",
        "\t\t# return the set of crops\n",
        "\t\treturn np.array(crops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NF8faSog8so"
      },
      "source": [
        "## 1.7 HDF5 dataset generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soLUcyH1iCOO"
      },
      "source": [
        "Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset, we first need to define a class responsible for yielding batches of images and labels from our HDF5 dataset. Section 1.1 discussed how to convert a set of images residing on disk into an HDF5 dataset – but how do we get them back out again? The answer is to define an **HDF5DatasetGenerator** class.\n",
        "\n",
        "Previously, all of our image datasets could be loaded into memory so we could rely on Keras generator utilities to yield our batches of images and corresponding labels. However, now that our datasets are too large to fit into memory, we need to handle implementing this generator ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMo22QKUg7me"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "class HDF5DatasetGenerator:\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None, aug=None, binarize=True, classes=2):\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\n",
        "\t\t# whether or not the labels should be binarized, along with\n",
        "\t\t# the total number of classes\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\t\tself.aug = aug\n",
        "\t\tself.binarize = binarize\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\t# open the HDF5 database for reading and determine the total\n",
        "\t\t# number of entries in the database\n",
        "\t\tself.db = h5py.File(dbPath, \"r\")\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
        "\n",
        "\tdef generator(self, passes=np.inf):\n",
        "\t\t# initialize the epoch count\n",
        "\t\tepochs = 0\n",
        "\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\n",
        "\t\t# reach the desired number of epochs\n",
        "\t\twhile epochs < passes:\n",
        "\t\t\t# loop over the HDF5 dataset\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
        "\n",
        "\t\t\t\t# check to see if the labels should be binarized\n",
        "\t\t\t\tif self.binarize:\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\n",
        "\t\t\t\t\t\tself.classes)\n",
        "\n",
        "\t\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t\t# initialize the list of processed images\n",
        "\t\t\t\t\tprocImages = []\n",
        "\n",
        "\t\t\t\t\t# loop over the images\n",
        "\t\t\t\t\tfor image in images:\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
        "\t\t\t\t\t\t# to the image\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t\t\t\t# update the list of processed images\n",
        "\t\t\t\t\t\tprocImages.append(image)\n",
        "\n",
        "\t\t\t\t\t# update the images array to be the processed\n",
        "\t\t\t\t\t# images\n",
        "\t\t\t\t\timages = np.array(procImages)\n",
        "\n",
        "\t\t\t\t# if the data augmenator exists, apply it\n",
        "\t\t\t\tif self.aug is not None:\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
        "\n",
        "\t\t\t\t# yield a tuple of images and labels\n",
        "\t\t\t\tyield (images, labels)\n",
        "\n",
        "\t\t\t# increment the total number of epochs\n",
        "\t\t\tepochs += 1\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# close the database\n",
        "\t\tself.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ou-_XnZqjrY"
      },
      "source": [
        "## 1.8 Simple preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juf1YRHzqmeU"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class SimplePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\n",
        "\t\t# ratio\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJUx9Prq2xE"
      },
      "source": [
        "## 1.9 Training monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1JGywYeq5Wj"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2HBr6JAMQ8"
      },
      "source": [
        "## 1.10 Rank accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJCbMd1sAPP8"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "\n",
        "def rank5_accuracy(preds, labels):\n",
        "\t# initialize the rank-1 and rank-5 accuracies\n",
        "\trank1 = 0\n",
        "\trank5 = 0\n",
        "\n",
        "\t# loop over the predictions and ground-truth labels\n",
        "\tfor (p, gt) in zip(preds, labels):\n",
        "\t\t# sort the probabilities by their index in descending\n",
        "\t\t# order so that the more confident guesses are at the\n",
        "\t\t# front of the list\n",
        "\t\tp = np.argsort(p)[::-1]\n",
        "\n",
        "\t\t# check if the ground-truth label is in the top-5\n",
        "\t\t# predictions\n",
        "\t\tif gt in p[:5]:\n",
        "\t\t\trank5 += 1\n",
        "\n",
        "\t\t# check to see if the ground-truth is the #1 prediction\n",
        "\t\tif gt == p[0]:\n",
        "\t\t\trank1 += 1\n",
        "\n",
        "\t# compute the final rank-1 and rank-5 accuracies\n",
        "\trank1 /= float(len(preds))\n",
        "\trank5 /= float(len(preds))\n",
        "\n",
        "\t# return a tuple of the rank-1 and rank-5 accuracies\n",
        "\treturn (rank1, rank5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrmCQKbMHI-4"
      },
      "source": [
        "## 1.11 SimpleDatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj4RbTs8HH62"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# helper to load images\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwDVpznk2rZ0"
      },
      "source": [
        "# 2.0 Working with HDFS and large datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esFTx_wHmni3"
      },
      "source": [
        "# download horsesvshumans dataset\n",
        "#!gdown https://drive.google.com/uc?id=1RqVl-1AOFRYaktJuG3xxxmuur0ua8s-p\n",
        "!gdown https://drive.google.com/uc?id=1JVo_ggZJbwEKgybgkSQfNZhL8_Xu5FFs&export=download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igquSvNn4G6"
      },
      "source": [
        "!unzip horsesvshumans.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCmYyH7NoDcS"
      },
      "source": [
        "!mkdir horsesvshumans\n",
        "!mkdir horsesvshumans/hdf5\n",
        "!mkdir horsesvshumans/output\n",
        "!mv horse-or-human horsesvshumans/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3OikZkoP8eU"
      },
      "source": [
        "!mv horsesvshumans/horse-or-human/train/humans/* horsesvshumans/horse-or-human/train/horses/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuv2WnpAQ0EU"
      },
      "source": [
        "!mv horsesvshumans/horse-or-human/validation/humans/* horsesvshumans/horse-or-human/validation/horses/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQjEjj79tCV5"
      },
      "source": [
        "ls horsesvshumans/train_test | wc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7hYvPJo4y_"
      },
      "source": [
        "# define the paths to the images directory\n",
        "IMAGES_PATH = \"horsesvshumans/horse-or-human/train/horses\"\n",
        "\n",
        "IMAGES_PATH_TEST = \"horsesvshumans/horse-or-human/validation/horses\"\n",
        "\n",
        "\n",
        "# since we do not have validation data or access to the testing\n",
        "# labels we need to take a number of images from the training\n",
        "# data and use them instead\n",
        "NUM_CLASSES = 2 #São 10 no nosso caso não ?\n",
        "NUM_VAL_IMAGES = 513\n",
        "NUM_TEST_IMAGES = 258\n",
        "\n",
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"horsesvshumans/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"horsesvshumans/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"horsesvshumans/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"horsesvshumans/alexnet_horsesvshumans.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"horsesvshumans/horsesvshumans_mean.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"horsesvshumans/output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omp0jxsm9Yr9"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('trainLabels.csv')\n",
        "\n",
        "# grab the paths to the images\n",
        "trainPaths = list(paths.list_images(IMAGES_PATH))\n",
        "# testPaths = list(paths.list_images(IMAGES_PATH_TEST))\n",
        "trainLabels = [p.split(os.path.sep)[-1].split(\"-\")[0][0:5] for p in trainPaths]\n",
        "\n",
        "testPaths = list(paths.list_images(IMAGES_PATH_TEST))\n",
        "testLabels = [p.split(os.path.sep)[-1].split(\"-\")[0][0:5] if p.split(os.path.sep)[-1].split(\"-\")[0] == \"horse\" else p.split(os.path.sep)[-1].split(\"-\")[0] == p.split(os.path.sep)[-1].split(\"-\")[0][3:8] for p in testPaths]\n",
        "print(testLabels);\n",
        "# trainLabels = df.label.tolist()\n",
        "# print(trainLabels)\n",
        "le = LabelEncoder()\n",
        "trainLabels = le.fit_transform(trainLabels)\n",
        "testLabels = le.fit_transform(testLabels)\n",
        "\n",
        "# df = pd.read_csv('sampleSubmission.csv')\n",
        "# testLabels = df.label.tolist()\n",
        "# le = LabelEncoder()\n",
        "# testLabels = le.fit_transform(testLabels)\n",
        "\n",
        "# perform stratified sampling from the training set to build the\n",
        "# testing split from the training data\n",
        "# split = train_test_split(testPaths, testLabels,\n",
        "#                          test_size=NUM_TEST_IMAGES, \n",
        "#                          stratify=testLabels,random_state=42)\n",
        "# (testPaths, testPathsMin, testLabels, testLabelsMin) = split\n",
        "\n",
        "# # perform another stratified sampling, this time to build the\n",
        "# # validation data\n",
        "split = train_test_split(trainPaths, trainLabels,\n",
        "                         test_size=NUM_VAL_IMAGES, \n",
        "                         stratify=trainLabels,random_state=42)\n",
        "(trainPaths, valPaths, trainLabels, valLabels) = split\n",
        "\n",
        "# construct a list pairing the training, validation, and testing\n",
        "# image paths along with their corresponding labels and output HDF5\n",
        "# files\n",
        "datasets = [\n",
        "  (\"test\", testPaths, testLabels, TEST_HDF5),\n",
        "\t(\"train\", trainPaths, trainLabels, TRAIN_HDF5),\n",
        "\t(\"val\", valPaths, valLabels, VAL_HDF5)]\n",
        "\n",
        "# initialize the image pre-processor and the lists of RGB channel\n",
        "# averages\n",
        "aap = AspectAwarePreprocessor(256, 256)\n",
        "(R, G, B) = ([], [], [])\n",
        "\n",
        "# loop over the dataset tuples\n",
        "for (dType, paths, labels, outputPath) in datasets:\n",
        "\t# create HDF5 writer\n",
        "\tprint(\"[INFO] building {}...\".format(outputPath))\n",
        "\twriter = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
        "\n",
        "\t# initialize the progress bar\n",
        "\twidgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\tpbar = progressbar.ProgressBar(maxval=len(paths),widgets=widgets).start()\n",
        "\n",
        "\t# loop over the image paths\n",
        "\tfor (i, (path, label)) in enumerate(zip(paths, labels)):\n",
        "\t\t# load the image and process it\n",
        "\t\timage = cv2.imread(path)\n",
        "\t\timage = aap.preprocess(image)\n",
        "\n",
        "\t\t# if we are building the training dataset, then compute the\n",
        "\t\t# mean of each channel in the image, then update the\n",
        "\t\t# respective lists\n",
        "\t\tif dType == \"train\":\n",
        "\t\t\t(b, g, r) = cv2.mean(image)[:3]\n",
        "\t\t\tR.append(r)\n",
        "\t\t\tG.append(g)\n",
        "\t\t\tB.append(b)\n",
        "\n",
        "\t\t# add the image and label # to the HDF5 dataset\n",
        "\t\twriter.add([image], [label])\n",
        "\t\tpbar.update(i)\n",
        "\n",
        "\t# close the HDF5 writer\n",
        "\tpbar.finish()\n",
        "\twriter.close()\n",
        "\n",
        "# construct a dictionary of averages, then serialize the means to a\n",
        "# JSON file\n",
        "print(\"[INFO] serializing means...\")\n",
        "D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n",
        "f = open(DATASET_MEAN, \"w\")\n",
        "f.write(json.dumps(D))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXIZpCslRci"
      },
      "source": [
        "!du -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUkad-ngIGv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0HOpKBS-pA7"
      },
      "source": [
        "ls horsesvshumans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgTT1MDgzUX"
      },
      "source": [
        "# copy hdf5 files for your google drive\n",
        "!cp -r horsesvshumans/hdf5 /content/drive/MyDrive/deep_learning_final/horsesvshumans #15/catdogs\n",
        "!cp -r horsesvshumans/horsesvshumans_mean.json /content/drive/MyDrive/deep_learning_final/horsesvshumans #15/catdogs\n",
        "!cp -r horsesvshumans/output/ /content/drive/MyDrive/deep_learning_final/horsesvshumans #15/catdogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFkeXcghlU6a"
      },
      "source": [
        "# 3.0 Competing in Kaggle: Humans X Horses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVqQ7et85T60"
      },
      "source": [
        "# \n",
        "# only in case load data from drive\n",
        "#\n",
        "# !mkdir horsesvshumans\n",
        "!cp -r /content/drive/MyDrive/deep_learning_final/horsesvshumans/hdf5 horsesvshumans/\n",
        "!cp -r /content/drive/MyDrive/deep_learning_final/horsesvshumans/output horsesvshumans/\n",
        "!cp /content/drive/MyDrive/deep_learning_final/horsesvshumans/horsesvshumans_mean.json horsesvshumans/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFt0U2oUmMuU"
      },
      "source": [
        "## 3.1 Implementing AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0EAsPgmEea"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class AlexNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, reg=0.0002):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# Block #1: first CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(96, (11, 11), strides=(4, 4),input_shape=inputShape, padding=\"valid\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #2: second CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(256, (5, 5), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #4: first set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# Block #5: second set of FC => RELU layers\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeWP4j7-nGuU"
      },
      "source": [
        "## 3.2 Training AlexNet on Kaggle: Humans x horses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXM6nqiaqBh1"
      },
      "source": [
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"horsesvshumans/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"horsesvshumans/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"horsesvshumans/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"horsesvshumans/alexnet_horsesvshumans.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"horsesvshumans/horsesvshumans_mean.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/deep_learning_final/horsesvshumans\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmNyJTrMpfeY"
      },
      "source": [
        "# import the necessary packages\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Start a new run\n",
        "wandb.init(project='human-horse-alex-net')\n",
        "\n",
        "# 2. Save model inputs and hyperparameters\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.01\n",
        "\n",
        "# Configurations related to checkpoint\n",
        "# resume or not the model\n",
        "resume = False\n",
        "\n",
        "# checkpoint files\n",
        "filepath= OUTPUT_PATH + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "pp = PatchPreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=2)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=2)\n",
        "\n",
        "# initialize the optimizer\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=1e-3)\n",
        "\n",
        "model = AlexNet.build(width=227, height=227, depth=3,classes=2, reg=0.0002)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "# construct the set of callbacks\n",
        "path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n",
        "callbacks = [TrainingMonitor(path)]\n",
        "\n",
        "initial_epoch = 1\n",
        "\n",
        "# load previou weights\n",
        "if resume == True:\n",
        "  model.load_weights(OUTPUT_PATH + \"/epochs:045-val_acc:0.921.hdf5\")\n",
        "  initial_epoch = 45\n",
        "\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 128,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 128,\n",
        "          epochs=20,\n",
        "          max_queue_size=10,\n",
        "          callbacks=[callbacks,checkpoint,WandbCallback()],\n",
        "          verbose=2,\n",
        "          initial_epoch=initial_epoch)\n",
        "\n",
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIhw-n3Dsrcu"
      },
      "source": [
        "**TIP**: Put this code in browser console in order to run the notebook for hours in Google Colab.\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgP_SX7JBvSK"
      },
      "source": [
        "## 3.3 Evaluating AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nruX7p86lq"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "cp = CropPreprocessor(227, 227)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the pretrained network\n",
        "print(\"[INFO] loading model...\")\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "# initialize the testing dataset generator, then make predictions on\n",
        "# the testing data\n",
        "print(\"[INFO] predicting on test data (no crops)...\")\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[sp, mp, iap], classes=2)\n",
        "predictions = model.predict(testGen.generator(),\n",
        "                            steps=testGen.numImages // 64, max_queue_size=10)\n",
        "\n",
        "# compute the rank-1 and rank-5 accuracies\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()\n",
        "\n",
        "\n",
        "# re-initialize the testing set generator, this time excluding the\n",
        "# `SimplePreprocessor`\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[mp], classes=2)\n",
        "predictions = []\n",
        "\n",
        "# initialize the progress bar\n",
        "widgets = [\"Evaluating: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64,widgets=widgets).start()\n",
        "\n",
        "# loop over a single pass of the test data\n",
        "for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):\n",
        "\t# loop over each of the individual images\n",
        "\tfor image in images:\n",
        "\t\t# apply the crop preprocessor to the image to generate 10\n",
        "\t\t# separate crops, then convert them from images to arrays\n",
        "\t\tcrops = cp.preprocess(image)\n",
        "\t\tcrops = np.array([iap.preprocess(c) for c in crops],\n",
        "\t\t\tdtype=\"float32\")\n",
        "\n",
        "\t\t# make predictions on the crops and then average them\n",
        "\t\t# together to obtain the final prediction\n",
        "\t\tpred = model.predict(crops)\n",
        "\t\tpredictions.append(pred.mean(axis=0))\n",
        "\n",
        "\t# update the progress bar\n",
        "\tpbar.update(i)\n",
        "\n",
        "# compute the rank-1 accuracy\n",
        "pbar.finish()\n",
        "print(\"[INFO] predicting on test data (with crops)...\")\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFwa9y74MEhy"
      },
      "source": [
        "## 3.4 Obtaining the top spot on the Kaggle Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_lpAJ7Nm7s"
      },
      "source": [
        "Of course, if you were to look at the Kaggle Dogs vs. Cats leaderboard, you would notice that to even break into the top-25 position we would need 96.98% accuracy, which our current method is not capable of reaching. So, what’s the solution?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcfAFunl6jY0"
      },
      "source": [
        "### 3.4.1 Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lequt3Gn7G0B"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQFxYXJ6RSrO"
      },
      "source": [
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"horsesvshumans/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"horsesvshumans/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"horsesvshumans/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"horsesvshumans/resnet50_horsesvshumans.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"horsesvshumans/horsesvshumans_mean.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm87KJ537Ead"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# whether or not to include top of CNN\n",
        "include_top = 0\n",
        "\n",
        "# load the ResNet50 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = ResNet50(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_xGqkF6lVJ"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# a fully connect network\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6e9nblD6x7z"
      },
      "source": [
        "# import the necessary packages\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Start a new run\n",
        "wandb.init(project='human-horse-alex-net2')\n",
        "\n",
        "# 2. Save model inputs and hyperparameters\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.01\n",
        "\n",
        "# Configurations related to checkpoint\n",
        "# resume or not the model\n",
        "resume = False\n",
        "\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(224, 224)\n",
        "pp = PatchPreprocessor(224, 224)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=2)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=2)\n",
        "\n",
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "num_classes = 2\n",
        "headModel = FCHeadNet.build(baseModel, num_classes, 256)\n",
        "\n",
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "# RMSprop is frequently used in situations where we need to quickly obtain\n",
        "# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 128,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 128,\n",
        "          epochs=20,\n",
        "          max_queue_size=10,\n",
        "          callbacks=[WandbCallback()],\n",
        "          verbose=1)\n",
        "\n",
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5TVOfzxJQy"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "# 1. Start a new run\n",
        "wandb.init(project='human-horse')\n",
        "\n",
        "# 2. Save model inputs and hyperparameters\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.01\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=2)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=2)\n",
        "\n",
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in baseModel.layers[165:]:\n",
        "\tlayer.trainable = True\n",
        "\n",
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] fine-tuning model...\")\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 128,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 128,\n",
        "          epochs=20,\n",
        "          max_queue_size=10,\n",
        "          callbacks=[WandbCallback()],\n",
        "          verbose=1)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj9IfD5Y6FII"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}